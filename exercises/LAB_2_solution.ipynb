{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e243096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp nlp True False PROPN (proper noun)\n",
      "is be True True AUX (auxiliary)\n",
      "fun fun True False ADJ (adjective)\n",
      "! ! False False PUNCT (punctuation)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"nlp is fun!\")\n",
    "for token in doc:\n",
    "    explanation = spacy.explain(token.pos_)\n",
    "    print(token.text, token.lemma_, token.is_alpha, token.is_stop, token.pos_, end=\" \")\n",
    "    print(f\"({explanation})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019cbb77-3d98-438a-a1d7-8071f5de9356",
   "metadata": {},
   "source": [
    "# 1) Text normalization\n",
    "\n",
    "First of all, removing too much from the words/tokens is dangerous, as this will have unwanted results for a keyboard. Punctuation can likely be omitted, as we can observe that commas and periods can occur after a large selection of words, with few obvious patterns.\n",
    "\n",
    "Stopword filtering is also not useful, as we want to predict words like \"is\", \"the\", etc.\n",
    "\n",
    "Lower/uppercasing, however, is likely a filtering step we can include. Lowercasing gives more data input and allows for more matches in your model.\n",
    "\n",
    "Lemmatization/stemming could be used in some cases, but this may depend on your domain and other factors. Consider the sentences \"I'm studying at NTNU\" and \"I often study late at night\". After \"study\", this could help the model predict both \"at\" and \"late\" as the next word.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e09f9da-8d82-441d-bc1e-426060a9a89f",
   "metadata": {},
   "source": [
    "# 2) TF-IDF\n",
    "## 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed4171fb-0743-4ea7-a06e-4d3f05b7ab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for 'love' is: 0.3010299956639812\n",
      "TF-IDF for 'like' is: 0.6020599913279624\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from math import log10\n",
    "\n",
    "def tf(document, term):\n",
    "    tokens = nltk.word_tokenize(document)\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    freq = nltk.FreqDist(tokens)\n",
    "    return freq[term]\n",
    "\n",
    "def idf(documents, term):\n",
    "    num_docs = len(documents)\n",
    "    num_docs_with_term = 0\n",
    "    for d in documents:\n",
    "        if term.lower() in d.lower():\n",
    "            num_docs_with_term += 1\n",
    "\n",
    "    return log10(num_docs / num_docs_with_term + 1)\n",
    "\n",
    "def tf_idf(all_documents, document, term):\n",
    "    _tf = tf(document, term)\n",
    "    _idf = idf(all_documents, term)\n",
    "    # print(f\"TF: {_tf}, IDF: {_idf}\")\n",
    "    return _tf * _idf\n",
    "\n",
    "d1 = \"I love cats\"\n",
    "d2 = \"I love dogs\"\n",
    "d3 = \"I love cats, but I also like dogs\"\n",
    "\n",
    "documents = [d1, d2, d3]\n",
    "\n",
    "print(f\"TF-IDF for 'love' is: {tf_idf(documents, d3, 'love')}\")\n",
    "print(f\"TF-IDF for 'like' is: {tf_idf(documents, d3, 'like')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "349ffc07",
   "metadata": {},
   "source": [
    "## 2.2) \n",
    "I would not replace words with their TF-IDF values without doing quite a bit of preprocessing first - such as extracting the part-of-speech tags, stems, etc. There's a lot of linguistic features to be extracted.\n",
    "\n",
    "TF-IDF values can, although, be used directly in certain applications, such as for search and information retrieval.\n",
    "\n",
    "\n",
    "## 2.3)\n",
    "You should use the logarithm of the inverse document frequency as you may end up with very large values if you have a big corpora to work with, containing e.g. million of documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "daef3a1b",
   "metadata": {},
   "source": [
    "# 3) Part-of-speech tagging\n",
    "We'll  now make use of spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e2b3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'), ('saw', 'VERB'), ('her', 'PRON'), ('duck', 'NOUN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I saw her duck\"\n",
    "\n",
    "nltk.pos_tag(nltk.word_tokenize(sentence), tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169300c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nltk.corpus.brown.tagged_sents(tagset=\"universal\")\n",
    "\n",
    "backoff = nltk.DefaultTagger('NN')\n",
    "bigramtagger = nltk.UnigramTagger(train=data, backoff=backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb57191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'), ('saw', 'VERB'), ('her', 'DET'), ('duck', 'VERB')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramtagger.tag(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6293c1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'), ('saw', 'VERB'), ('her', 'PRON'), ('duck', 'NOUN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentence)\n",
    "[(token.text, token.pos_) for token in doc]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0bdbc21",
   "metadata": {},
   "source": [
    "Using only the unigramtagger, we don't get a lot of context together with the word \"duck\", and apparently the corpus more often than not has the word \"duck\" used as a verb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "29f1e52c0d8d5e5ede6aaca4be8238d35b46afd62a3b8286547e2768de775769"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
